\section{Netronome NFP-6000}

\abbr{FPC}{Flow Processing Core}

As the last examined controller, we have chosen Netronome NFP-6000. The design
of the NFP controller series is presumably completely different from the other
vendors. Instead of being a highly specialized packet processing silicon, the
controller is a programmable parallel compute unit with features for packet
processing and network connectivity. The Netronome controllers NFP-4000 and
NFP-6000 share the same architecture, but differ in the number of functional
units used.

The information in this section is based mostly on papers published by Netronome
\cite{nfp-4k-too,nfp-prm,nfp-micro-c}, with a few bits deduced from the Linux
kernel source code.

The controller is used exclusively in the adapters by Netronome. It is attached to
the host via up to four independent PCIe Gen 3 x8 interfaces, connecting to up
to four CPU sockets in one host. They can handle up to four 100 Gbps Ethernet
interfaces, with both integrated \a{MAC} or a transmitter connected via SFP+,
QSPF or CXP.

The controller is composed from so-called islands, which have isolated
responsibilities. Most of the islands are connected with a bus.
The architecture is modular, allowing to create
different versions of the same controller design.

The islands include:

\begin{description}
	\item[Ingress MAC and Packet Processing] \hfill \\
		Receives packets from the network interface. Parses headers, verifies
		checksums and constructs packet metadata. The packet payload is sent to
		the memory units, the packet metadata to the Flow Processing islands.
	\item[Flow Processing] \hfill \\
		Performs arbitrary processing of packets using the packet metadata.
	\item[Egress Packet Processing and \a{MAC}] \hfill \\
		Reorders packets from the same flows and schedules them to the network.
	\item[ARM Subsystem] \hfill \\
		Contains a fully-featured ARM processor, which is able to run Linux.
		This processor can be used to configure or monitor the controller as
		well as run any other application.
	\item[Crypto] \hfill \\
		Specialized circuits to support the Flow Processing units in encryption and
		decryption of arbitrary data.
	\item[Memory units] \hfill \\
		Globally-accessible memory units to store tables or data into. Apart
		from up to 30 MB of on-chip memory, there is an external memory unit which
		supports up to 24 GB off-chip DDR 3.
	\item[PCIe] \hfill \\
		Used for communication with the host. Packets sent by the host are
		processed similarly to ingress packets, the payload is stored in the
		memory units and metadata is passed to Flow Processing islands.
\end{description}

\noindent An important building block is the \acrfull{FPC}, which is a programmable 32-bit
processor core designed for packet processing. The core runs up to 8 threads,
which are cooperatively scheduled when waiting for data, similarly to threads
on \a{GPU}s.

The flow processing islands are made of 12 \a{FPC}s each. They do the most of the
packet processing work. Apart from forming the Flow processing islands,
\a{FPC}s are present in lower numbers in other islands as well, making even the
fixed parts of the pipeline programmable.

The \a{FPC}s can be programmed using an open source \a{SDK} to perform any
processing. The \a{SDK} provides a framework to program packet processing
using the \a{P4} or C language, or allows to write programs running on bare
metal.

The processor is not fully featured, it has a simple architecture (e.g.\ cannot
calculate with floating point numbers, there is no stack and so on), but
certainly is more flexible than a match-action pipeline. The controller is
therefore able to do any packet processing for any application, provided the
program fits into the instruction memory.

If not configured and programmed by the user, the controller ships with
firmware that emulates the behavior of a conventional \a{NIC}. The firmware
offers several ``apps'', which define the capabilities of the controller from
the operating system point of view.

As the capabilities of this controller are implemented by software, it does not
make sense to describe the controller in terms of the current firmware version. We may
assume that the controller fits well into any reasonable offload model.

Taken to the extreme, the controller is capable of running Open vSwitch by
itself, on the \a{ARM} processor, offloading the heavy lifting to the \a{FPC}s,
both without the host intervention. The ports of the virtual switch still
correspond to individual network interfaces presented to the host, creating
a very unusual scenario of a separate machine running Open vSwitch, connected
via PCIe to the host.
