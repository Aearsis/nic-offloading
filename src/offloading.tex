\chapter{Hardware Offloading in General}
\label{chap:offload}

In this chapter we would like to introduce the current techniques used to reduce
the amount of work that the network stack has to do in software (by the host CPU)
by doing it in the \a{NIC} instead. The simplest techniques are implemented by
virtually every \a{NIC} currently on the market, the interfaces are stable and
well-supported. On the other end of the spectrum there are techniques which
are specific to high-performance cards and are under rapid development at the
time of writing.

All of the information given in this chapter is public. However, there is no
comprehensive overview of available techniques. Individual offloads are given
business names and the implementation is buried deep inside datasheets.
Available documentation for drivers and operating systems focuses on how to
control the mechanisms and when it is good to use them, but usually do not
cover the principles behind. Therefore, this chapter aims to be an introduction
to modern networking world.

It is worth noting that every vendor of a network controller uses their
specific terminology. The implementation details might differ as well, but the
high-level principles described in this chapter are implemented by multiple
\a{NIC} vendors.

An important aspect of an \emph{offload} is that the functionality provided by
the hardware is not required for frames to be processed. The functionality is
strictly opt-in and only if the driver of the device is capable of doing so, it
can enable it and benefit from skipping some steps in the network frame
processing on the host.

At first, we would like to mention two techniques that can be
classified as offloads, though this requires a rather loose interpretation of the
term. First is the scatter-gather capability of the controller, which frees the
host from assembling multiple buffers into one. Second is the interrupt
moderation, which reduces the overhead of switching contexts and allows to
batch-process multiple frames. Both of these techniques save some CPU cycles to
process a frame, but they do not depend on the frame content.

\section{Checksum Offload}

A common approach to ensure consistency of data transmitted over a network
involves checksums. Those are values that are usually inexpensive to compute,
yet provide a good level of reliability in indicating data corruption.

At the link layer the \a{NIC} usually computes the checksum itself, because it
is well aware of the protocol being used. In the case of Ethernet the
checksum is called \emph{Frame Check Sequence} and is transmitted at the end of
the frame, after the payload \cite{ethernet}. Therefore, the \a{NIC} is able to
compute the intermediate value of the checksum while sending and finish the frame
by the final checksum. Similarly, on the ingress path, after a frame is received
completely (with the checksum included), the value should be zero -- otherwise
the frame is dropped.

Moving up to the network layer, \a{IPv6} does not embody a checksum at
all \cite{RFC2460}. The \a{IPv4} header contains a one's complement checksum of the
header itself \cite{RFC0791}. The header is built completely by the software
and its maximum size is 60 bytes,
therefore the checksum is not expensive to compute. Still, many controllers are
capable of computing the \a{IPv4} header checksum before transmission.

Matters get complicated at the transport layer. A handful of protocols uses 16-bit
one's complement checksum of the whole packet -- not only \a{TCP} and \a{UDP},
the same approach is used by \a{DCCP} or \a{GRE}, for example. The offset at
which the checksum field is placed in the header varies with the protocol.

Both receive and send offloading capabilities for the transport layer checksums are
commonly offered by \a{NIC}s. On the receiving side, a \a{NIC} can either
validate the packet directly or just calculate the checksum including the
checksum field. The second way is preferred because of its flexibility, as we
will see in detail in Section \ref{tag:linux-rxcsum}.

Due to the algebraic properties of the checksum, it is not necessary to know
where the checksum field is to verify the received packet, making the receive
checksum offload both simple and protocol-agnostic. However, the
transport-layer packet must be received completely, in other words, the \a{IP}
packet must not be fragmented.

When sending a packet, older \a{NIC}s might be able to only check for
the presence of a known transport protocol and compute the checksum for them.
Recent controllers have the ability to compute checksum of any suffix of the
packet and write it to an arbitrary offset, allowing the system software to
offload compatible protocols in a generic way.

Both \a{TCP} and \a{UDP} compute the checksum not only from the packet header
and payload, but also from a \emph{Pseudo Header} which is composed of the source and
destination addresses.
These are not duplicated in the transport layer \a{PDU}, and because of that
the device might require the system software to precompute the checksum of the pseudo
header, exploiting the associativity and commutativity of the checksum
algorithm. An example of this requirement can be found in the Intel XL710
Controller (\cite{XL710}, Section 8.4.4.3.2). When a packet is received, its
computed checksum (including the checksum field) should be equal to
a complement of the pseudo header checksum.

For different checksumming algorithms, alternatives are also often provided.
For example, the Intel 82599 Controller can offload \a{SCTP} \a{CRC}
(\cite{82599}, Section 7.2.5.3).

\section{\a{TCP} Segmentation Offload}

As \a{TCP} is offering an interface of a stream pipe, it includes is a rather
complicated mechanism which creates the illusion of the pipe on top of a network that is only able to
transmit individual packets of bounded length.
When chunks of data are inserted into the pipe, they are split into \emph{segments}.
Segments are labeled with sequential numbers and sent individually. The
sequential numbering is used to ensure that no segments are lost and data is
delivered (passed to the application layer) in the correct order.
However, the size of the chunks is not defined. Taken to the extreme, \a{TCP}
could deliver individual bytes.

Obviously, a lot of overhead can be mitigated by handling data in the biggest chunks
possible, traversing the entire stack the least number of times. Speaking about
software, the network stack usually performs the segmentation at the latest
possible moment, performing as much processing as possible in batches. When receiving, the stack
might try to coalesce segments of a single \a{TCP} stream before delivering them.

This effort can be further supported by the hardware. A \a{NIC} can offer an
interface which allows to enqueue \a{TCP} packets that are larger than the link
\a{MTU}. Before transmitting them on the wire, the controller splits the packet
into multiple segments by itself. This way, the network stack is no longer
bound to the link \a{MTU} and can work with coarser chunks. This feature is
often referred to as \emph{Large Send Offloading}.

As this offload needs to have insight up to the \a{TCP} header, only a limited
set of header combinations is typically supported. Apart from plain \a{TCP} over
\a{IP}, more advanced \a{NIC}s can support \a{TCP} encapsulated in multiple
tunnel types.

The receiving counterpart of \a{LSO} is usually called \acrfull{LRO} and
does the expected opposite. Prior to enqueueing the packet into the receive
queue of the operating system, it tries to coalesce multiple received segments
belonging to one \a{TCP} stream into one super-frame. It is important to make
this feature optional, because it would violate the responsibilities of a network
when the packet is bridged or routed in the software.

Careful reader might notice that both directions of \a{TCP} segmentation offload depend on
checksum offload, as the newly-forged packets containing segments need to have their checksum
computed by the device on all layers. On the ingress path, the to-be-forgotten
packets carrying segments need to be verified prior to coalescing, and the
coalesced packet either needs to have a valid checksum or it must be marked in
order to skip verification by the software stack.

The checksums are the key issue the controller has to solve to support
segmentation offloads over tunnelling protocols, as multiple checksums might
need to be computed along the way to the inner \a{TCP} packet, while it is the
\a{TCP} packet payload being segmented. Therefore, not only the \a{TCP}
checksum, but also the outer \a{UDP} checksum changes for e.g.\ \a{VXLAN}.

As a side note, \a{TSO} can increase performance in virtualized environments,
as it increases effective \a{MTU} of the virtual links without
breaking isolation.

\section{UDP Fragmentation Offload}

A similar but much simpler technique exists for \a{UDP} datagrams. The length of one
\a{UDP} datagram is limited to 64~KB, larger than \a{MTU} of common links.
\a{UDP} itself does not define any concept of fragmentation, instead it
utilizes \a{IP} fragmentation to deliver payloads larger than \a{MTU}.

Controllers might be capable of performing \a{IP} packet fragmentation on the
chip. In contrast to \a{TCP} segmentation offload, \a{IP} fragmentation
can be done without touching the transport layer \a{PDU}, thus only the \a{IPv4} header
checksum must be updated.

The offload techniques described so far are often referred to as \emph{Stateless
Offloads} despite the fact that e.g.\ \a{LRO} needs to maintain state. Because
of its convenience, we will use this terminology as well.

\section{TCP Offload Engine}

Some \a{NIC}s are equipped with a full \a{TCP} stack Offload Engine (TOE). Using this
engine, the host leaves full \a{TCP}/\a{IP} stack processing on the \a{NIC}.
There are two main approaches. In the less invasive one, the software stack
initiates the connection using regular mechanisms, then hands the stream over
to the \a{NIC}. From that moment, the \a{NIC} offers a complete stream
interface, and handles all the \a{TCP}-related work -- segmentation, congestion
control, retransmissions and so on.

The second and even more intrusive approach leaves the \a{TCP} connection
handling on the hardware from the very beginning. Essentially, the \a{TOE} driver
replaces the \a{TCP} stack completely.

While \a{TOE} might seem superior to partial offloads, it brings a lot of
controversy. Processing \a{TCP} is not a simple task at all, and complex code
cannot avoid bugs and security flaws. Updating a kernel is a simple task
compared to updating a firmware in a controller. Furthermore, the operating
system cannot control the extent of features provided by the \a{TOE}, and users
would probably be confused by missing features like firewall or \a{QoS}. This holds even more so
when we consider that these features are still configurable, but have no effect
on the offloaded \a{TCP} stack, because the respective packets bypass them.

\section{Multiple Queues}

A lot of opportunities to improve performance arise from creating multiple
queues for the communication between the host and the \a{NIC}. The maximum number
of queues in question is specific to the controller and depends on the
associated purpose. As we will see, multiple
queues can help in more cases than only on multi-processor systems.

\subsection{Multiple Send Queues}

Let us start with increasing the number of egress queues per port. To handle
multiple queues, the controller must multiplex them on the wire. The algorithm for
multiplexing frames from multiple queues in fact performs frame scheduling.

With the knowledge of the scheduling algorithm, the host can offload scheduling
to the \a{NIC} by selecting the proper queue for every frame. This requires
both the algorithm and the number of queues to be compatible with the desired
behavior.

We will show two examples of how multiple queues can be used. In the first one, assume that
the scheduling algorithm is round-robin, thus every queue is treated equally
(with respect to the number of frames, not bytes sent). In a sense, the host can
treat every queue as a separate interface to a shared medium. On
a multiprocessor system, it is possible to dedicate one queue to every core,
offloading the synchronization between the cores to the \a{NIC}. The downside of
this approach is that the bandwidth is not equally and deterministically divided
between processes, instead it is affected by the number of threads and thread
scheduling.

In the other example, consider a strict-priority scheduling algorithm. In this
model, the host can differentiate several traffic classes (e.g.\ prioritize
interactive traffic before bulk traffic) and distribute them among egress
queues appropriately. This technique can cut a bit of latency introduced by
buffering in the hardware queues of the device. Some scheduling still needs to
be done in the software, as there can be multiple sources of frames which
need to be multiplexed into one queue.

Scheduling offload is very hard to adopt, because subtle differences between
scheduling algorithms of the host and \a{NIC} can violate the intent of the
system administrator. The \a{NIC} driver must make sure that the current
scheduling algorithm is compatible with what the \a{NIC} employs.

Recent controllers offer some flexibility in terms of configuration of the
scheduling algorithm. For example, the Intel XL710 controller \cite{XL710}
defines queue sets, which can be arranged to a tree. Leaf nodes are queue sets
tat can be arranged in a tree. The leaf nodes are queue sets,
the inner nodes select its children in a configurable combination of weighted
strict priority and weighted round robin order. The bandwidth is distributed
equally among the queues in a set.

\subsection{Multiple Receive Queues}

Multiple ingress queues can be utilized quite
easily. The controller has to distribute the frames received from the wire to
the queues and the selection of algorithm the to do so creates space for offloads.

\subsubsection{Receive-Side Scaling}

To take advantage of multiple processors available for processing
network traffic, a simple mechanism called \emph{Receive-Side Scaling} was
specified by Microsoft in the Network Driver Interface Specification \cite{NDIS}.

The idea is that similarly processed frames should be handled by the same
\a{CPU} to maximize the benefits of caching. Also, frames from a single flow are
likely to be processed similarly. Therefore, an \a{RSS}-enabled \a{NIC}
extracts the source and destination addresses, possibly also the \a{TCP}/\a{UDP}
source and destination ports, and calculates a hash function of those. The lower
bits of the result are used to select the target queue. Queues are then uniformly
distributed among \a{CPU}s and the interrupt affinity is set accordingly.

One might dispute \a{RSS} being considered an offload, as it does not have a
software predecessor. We can take \a{RSS} as a reason why multiple receive queues
exists and as an inspiration for all other multi-queue offloads.

\subsubsection{Differentiated Services}

Instead of merely distributing frames in a stochastic manner, the queues can be
assigned a specific purpose. Every \a{IP} packet carries a \acrfull{TOS} field,
which can be used to assign a traffic class to every frame. Traffic classes can
then be mapped to receive queues by the \a{NIC}. Knowledge of the priority
mapping then allows the software stack to handle certain traffic classes with higher
priority than the others.

Differentiating services as soon as possible is important under heavy load. For
example, \a{TCP} avoids congestion by lowering the transmission rate when a segment is lost.
This means that the rate is only lowered when some network element decides to drop
a packet. Unless some more sophisticated algorithm to drop packets earlier is
employed, packets are dropped only when a queue is overfilled. That results in
queues being kept rather full, introducing delays for high-priority traffic as
well.

\subsubsection{Advanced Classification}
\label{offload:classification}

The \a{NIC} can select the target receive queue based on a more complex
algorithm, considering not only the \a{TOS} field in the \a{IP} header, but
also other fields. Usually, the set of fields the controller is capable of
matching on is limited. The variants might be mutually exclusive or fixed in
linear order of execution. Popular variants include:

\begin{itemize}
	\item Ethertype
	\item Source or destination \a{MAC} address
	\item Transport layer 5-tuple (protocol, source and destination address,
	      source and destination port)
	\item \a{VLAN} ID or other tunneling header fields
\end{itemize}

In addition to the predefined header fields, the \a{NIC} can offer a way to define
new header fields. The number of configurable fields tends to be very low. On the
other hand, several switch \a{ASIC}s are equipped with packet parsers fully
programmable by firmware updates, and therefore we can expect similar
flexibility in \a{NIC} controllers in a foreseeable future.

Usually, the matching rules are arranged into tables. There can be either a fixed pipeline of
tables, or a table hierarchy might be defined at runtime, or a combination of
both. Every rule carries a queue number, which is used whenever a packet
matches the rule.

Multiple modes of matching on a field can be distinguished. First, the field
might be examined for an exact value. Or, every rule can contain a \emph{value}
and a \emph{mask}. Rarely, the fields can be compared with ranges or
a longest-prefix match can be selected.

In the exact-match mode, a controller can use a special type of memory called
\acrfull{CAM} -- a memory where lookup of a row with a given value is done on all
rows in parallel. Or, it can use hash tables and store the tables in \a{RAM},
but that requires handling collisions.

The mask-value rules are used frequently in the networking world, e.g.\ for routing. For every rule
individually, the field value is first masked by the mask and then compared
with the expected value. In hardware, this operation can be realized on all
rules in parallel by using a \acrfull{TCAM} -- a memory addressable by keys in which
the rules can ignore individual bits. However, this requires the memory to store at
least three states per bit. Therefore, the flexibility is paid for with
much higher number of transistors for the same size of the table. Due to their
construction, \a{TCAM}s also consume a lot of power and take up a lot of space
on the chip. Therefore, tables placed in \a{TCAM} are usually a lot more
constrained than tables in \a{CAM} or \a{RAM} -- in both the total size of the
used fields and the number of rows.

Range matching can be implemented with a \a{TCAM} extended for this purpose
or mapped to regular \a{TCAM} matches. Such mapping can consume as much as $2
\cdotop \lceil \log_2 (B - A) \rceil$ \a{TCAM} rules to represent $x \in (A,
B)$ rule. Longest prefix matching is usually implemented by \a{TCAM}s with
priority such that the lookup result is always the first matching rule. Then,
ordering prefixes from the longest to the shortest makes \a{TCAM} perform
a longest-prefix match.

\section{Flow Offload}
\label{offload:flow}

In the last few years, controllers of the network interfaces started to resemble
switch controllers. It might seem strange at first, as \a{NIC}s have usually
very few external ports compared to switches. However, modern high-performance
\a{NIC}s usually feature \acrfull{SR-IOV}, which presents the controller as
multiple \a{PCIe} devices to the host. Usually, the first device is called
\emph{physical} function, the others \emph{virtual} functions. The virtual
functions can be handed out to virtual machines, removing the overhead of
communicating through the hypervisor. Usually, the virtual functions can be
restricted or partially configured via the physical function, supporting
virtualization deployments.

The \a{SR-IOV} can be seen as multiple virtual
ports going out from the \a{NIC}, naturally creating the need for switching.
The usual scheme is that the adapter external ports as well as the physical and
virtual functions are connected as external ports to an inner switch. Such
requirement becomes even more apparent with multi-host network adapters, such
as the Mellanox ConnectX-5 EN \cite{mlx5-pb} based adapters.

The existence of an inner switch is not hidden by the controller. Inner switches
are usually not that performant and flexible as fully-fledged switch controllers,
but we can already observe inner switches adopting useful features and
complexity of standalone switches.

When it comes to virtualization, a \acrfull{SDN} is usually
deployed. The ``software defined'' aspect means that the physical topology is
hidden, creating a virtual overlay network over the physical one.
To give an example, consider a data center running virtual machines, that are
migrated to balance load. Traditional network elements would react too
slowly to enable this scenario, so an externally configurable, more flexible
switch is used instead. The distributed nature of forwarding is suppressed by
a controller that controls multiple switches in the network and plans paths
for packet flows. \a{SDN}-configured switches do not try to find paths by
themselves, they need to be programmed by the controller explicitly. The
communication between the switch and the controller is realized by a protocol
designed for this purpose -- for example the OpenFlow protocol \cite{openflow}.

To perform forwarding in an \a{SDN} environments, switches feature a flow table. When
an unknown packet is received, it is forwarded to the controller. It then
identifies the flow and installs a rule into the switch flow table. The next time
a packet from this flow is received, it is forwarded without controller
intervention. As the memory of the switch is limited, unused flow table entries
are evicted.

This model is commonly described as a layered one, the controller being
a control plane and the forwarder being a data plane. While the control plane
focuses on flexibility and features, the data plane focuses on performance. This
separation can be observed in many instances in the networking world.

So far, the inner switches in \a{NIC}s do not have the required configurability
(e.g.\ they cannot communicate with the controller using the OpenFlow protocol)
to fully support \a{SDN}. Instead, software switches are being deployed. This
opens an opportunity for offloading, which is slowly being implemented by the
\a{NIC}s. Controllers with \a{SR-IOV} have the ability to offload the virtual
function selection.

To a person with theoretical education in networking, switching is a matter of
decision based on the destination \a{MAC} address, and all of this might seem
as an over-engineered solution. Nevertheless the destination \a{MAC} address is no
longer the only field used for selecting the destination port. Usually, the same
classification engine used for steering packets among receive queues is used
for the target port selection. As the receive queue is bound to one port, both
switching and receive queue selection can happen at the same time, such
a solution is not common though.

\subsection{\a{VLAN} and Tunnel Offload}

Another new responsibility of switches is tunnel handling. For several years, \a{VLAN}-handling
features can be found even in \a{SOHO} switches. The switch can
transparently emulate multiple networks by adding and removing \a{VLAN} tags on
the client ports. To improve virtual network isolation, the port traffic
can be usually filtered based on the \a{VLAN} tags. Similar features with different
tunneling protocols can be found in high-end switches.

When a host runs multiple virtual machines, it probably needs to be connected
to multiple virtual networks. In other words, the tunnel processing must be
moved from the standalone switch to the switch inside the host -- either the
software one, or the inner switch inside a \a{NIC}.

Presence of tunnelling prevents the flow offloading described in Section \ref{offload:flow},
because the virtual machine would see packets wrapped in the tunnel header.
Therefore, filtering and encapsulation/decapsulation of
tunnels must be done as well to enable flow offloading for these scenarios.

The problem of tunnel offloading is its future compatibility and flexibility.
It takes a lot of time since the controller is designed until it is used in
a network adapter available on the market. Furthermore, the card is expected to
last a few years. On the other hand, new tunnelling protocols are still being
developed (e.g.\ the Geneve protocol, first drafted in 2014, is still not
standardized but is in active development \cite{ietf-nvo3-geneve-06}). The
functionality of tunnel engines can be superseded by programmable actions
described in Section \ref{offload:match-action}.

\subsection{Access Control Lists}

Together with switching and/or advanced classification, access control can
be implemented in hardware. With all the classification apparatus, dropping
matched flows is just a simple extension to switching or queue selection.

Let us show one example where offloading the \a{ACL}s can play a significant
role -- a \a{DoS} protection. Suppose a server providing a service, which is
under \a{DoS} attack. Incoming requests undergo accounting that can consider
arbitrary properties of the request (source address, source subnets, or even
some domain-specific properties like the \a{API} key being used). Flows
exceeding the configured rate are considered malicious and a rule dropping the
frames of this particular flow is installed. From now on, the traffic from the
attackers can no longer disrupt the service, because the packets are dropped
before reaching the first software component.

This scenario could be handled even better by dropping the malicious flows earlier, for
example on the boundary router. But the flexibility and cost-effectivity of the
described solution is a thing to consider.

\subsection{Match-Action Offload}
\label{offload:match-action}

All the advanced classification offloads described so far can be further
generalized by moving closer to the hardware implementation.

A packet processor usually features a pipeline that can be described as
follows. When a packet arrives, it is parsed by a parser. Metadata and
individual header fields are extracted to a structure following the packet
through the pipeline. Then, a series of match-action steps is performed. In
every step, the packet is matched to a table previously described in Section
\ref{offload:classification}. From there, a Match
Index is obtained and used to lookup an entry in an action table, which
contains a chain of actions for every Match Index possible. Actions are usually
operations on the metadata (set field, copy field value to another field, etc.)
or the packet as a whole (drop, pop header). The action results might be
reflected in the packet data directly, or just in the metadata carried around.
At the end of the match-action pipeline, there is a deparser, which moves
the changed metadata back to the packet to reflect all actions. The last step is
necessary to avoid touching the packet data directly from the pipeline, which
is complicated.

This claim of the previous paragraph is supported by the existence and spread of the OpenFlow \cite{openflow}
protocol. The protocol serves for communication between switches and \a{SDN}
controllers, as explained in Section \ref{offload:flow}. The protocol operates
on tables, header fields and actions, and assumes that the switch packet
processing pipeline has the architecture described.

In the most common case the processing pipeline is fixed. It might be
linear or have branches, but usually cannot contain cycles. Every table in
a fixed pipeline has a predefined set of headers matched (and the mode of
matching), and a set of actions allowed.

For example, the switching decision of an ordinary switch can be described as a table
performing a lookup on destination \a{MAC} address. If the address is known,
the specified outgoing port is used. If not, the packet is mirrored to all
ports on behalf of the default action.

As another example, we can take the \a{VLAN} engine of a simple switch. This step
is performed in the egress pipeline of a port (specifically, after the forwarding database
is consulted). The table matches on the \a{VLAN} ID. When the engine is turned
off, the table is empty and the default action is to pass the packet without
modifications. If the engine is turned on for the port, the default action is
to drop the packet. When the port is connected to a trunk link, the table matches
\a{VLAN} IDs of the networks the trunk link belongs to, and the action for
all of them is to pass without modifications. If, on the other hand, the link is
connected to a host unaware of the virtual networks, the only entry in the table
matches the \a{VLAN} the host is connected to and the action is to pop the \a{VLAN}
header. Remember that the default action remains to drop the packet, so the
engine performs filtering in the same step as well.

Some switch silicons have the pipeline at least partially programmable.
The software can define a new graph of tables, which is inserted into the
pipeline. Those tables can match an arbitrary subset of headers matched and can be
used to offload almost any real-world packet processing scenario of known
protocols.

The programmability of the pipeline alone is not sufficient to implement
protocol-agnostic packet processing. For that to be possible, the parser needs
to extract fields from the unknown protocol to match on them. The OpenFlow
protocol (in version 1.5) defines a set of matchable fields, which
cannot be extended. That limits its usage for switches with programmable
pipelines \cite{openflow}.

As stated earlier, there are several switch controllers on the market that are
equipped with a parser programmable by firmware updates. Such a controller can be
used to implement the processing of protocols that are invented later than the
switch, or application-specific protocols in general.

And how does that relate to \a{NIC}s? We have seen many cases of \a{NIC}s adopting
features from switch drivers, and we expect that this is going to be another
one. Supporting this claim is the Mellanox ConnectX-5 controller, which already
supports the creation of a graph of tables which do match-action processing of
the flow (see Section \ref{mlx:pipeline}).



% Removed paragraphs for easier access:
%
% The pipeline can be formally described in the \a{P4} language\cite{P4}.
% A program in \a{P4} specifies two aspects of the pipeline: the header parser
% and the match-action pipeline. The language is expressive enough to describe
% many existing fixed pipelines, yet constrained enough to allow implementation
% of a compiler to partially-programmable pipeline configuration. There is
% a handful of \a{P4} compiler backends at the time of writing. The language was
% designed with both contemporary and future hardware in mind.
% 
% One of the software switches being deployed is Open vSwitch. The software
% system consists of a userspace daemon and a kernel module. The kernel module
% serves as a dumb forwarder, which forwards according to its \acrfull{FDB}. The
% kernel module however does not learn -- frames which are unknown to the \a{FDB}
% are trapped up to the userspace daemon. The daemon can be arbitrarily complex,
% because it runs in userspace. After the daemon decides what to do with the
% frame, it can install a flow into the kernel -- insert an entry into the
% \a{FDB}. From now on, the frames belonging to the flow are forwarded by the
% simple algorithm in kernel, which is faster.
%
% This model is not specific to software switches, actually, it was used in
% advanced hardware switches first. The layers are usually called a \emph{Data
% plane} and \emph{Control plane}.

% The
% industry developed such that (not only) data center providers need to load
% balance with respect to source IP address, differentiate servers according to
% transport-layer ports, forward traffic according to tunnelling protocols and
% so. Sure, all of these can be done ``properly'' following the layered network
% model, but a shift towards distributed and more powerful switching brings
% higher throughput and shorter, more deterministic latencies is apparent.
% 
% Still, we have not mentioned how this relates to offloading -- everything was
% purely about software yet. The thing is that switch controllers (such as in
% Mellanox Spectrum) might offer an interface, through which the software can
% install flows directly into the hardware, and the frames can be then forwarded
% without software intervention. As the resources in hardware are limited, the
% kernel module must be preserved to handle excess flows. One can say that the
% hardware is a data plane for which the kernel serves as a control plane. The
% Spectrum switch in question essentially behaves like a bunch of \a{NIC}s, which
% are capable of communicating together without any host assistance.
% 
% Similar trend can be observed in the \a{NIC} world as well. Controllers often
% possess a mechanism for fine-grained control of switching. Its implementation is
% usually joined with the classification engine explained above -- the receive
% queue selection is just extended with port selection. It is worth noting that
% this mechanism can be specific for ingress traffic from the adapter point of
% view, and does not affect egress traffic.
